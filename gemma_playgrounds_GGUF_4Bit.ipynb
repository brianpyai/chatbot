{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianpyai/chatbot/blob/main/gemma_playgrounds_GGUF_4Bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-czlAOa2dN4g",
        "outputId": "2a62e881-7820-4fcd-af8e-8831a88918dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio==4.22.0\n",
            "  Downloading gradio-4.22.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.22.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (4.2.2)\n",
            "Collecting fastapi (from gradio==4.22.0)\n",
            "  Downloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==4.22.0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.13.0 (from gradio==4.22.0)\n",
            "  Downloading gradio_client-0.13.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio==4.22.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio==4.22.0)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.7.0)\n",
            "Collecting pydub (from gradio==4.22.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio==4.22.0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio==4.22.0)\n",
            "  Downloading ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==4.22.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.22.0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (4.11.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==4.22.0)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.13.0->gradio==4.22.0) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.13.0->gradio==4.22.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.22.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.22.0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.22.0) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio==4.22.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio==4.22.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.22.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.22.0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.22.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.22.0) (2.18.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.22.0) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio==4.22.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio==4.22.0)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.22.0) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio==4.22.0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.22.0) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.22.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.22.0) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio==4.22.0) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.22.0) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=e3fb80191af3d5f766b214a8c4c8a40c95d2b6f74d4926b04554abb823c09dc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.2 ffmpy-0.3.2 gradio-4.22.0 gradio-client-0.13.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.1 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gradio==4.22.0 transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "vJ8HCwYutHId",
        "outputId": "f5cef95e-e534-43e3-bf2e-9fce29c2ff2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#@title Built llama-cpp-python with BLAS or CUDA automatically, wait for few minutes .\n",
        "import os,torch\n",
        "\n",
        "\n",
        "if torch.cuda.is_available ():\n",
        "    cmd='CMAKE_ARGS=\"%s\" pip install llama-cpp-python==0.2.62' % \"-DLLAMA_CUBLAS=on\"\n",
        "else:\n",
        "    cmd='CMAKE_ARGS=\"%s\" pip install llama-cpp-python==0.2.62' % \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"\n",
        "\n",
        "os.system (cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOxIBjXDN2ee",
        "outputId": "350ba9c0-f192-4354-8ffc-dde2b41cf4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./Llama3-8B-DPO-uncensored.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama3-8B-DPO-uncensored.Q8_0.gguf mradermacher/Llama3-8B-DPO-uncensored-GGUF\n",
            "mradermacher/Llama3-8B-DPO-uncensored-GGUF Llama3-8B-DPO-uncensored.Q8_0.gguf True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  8137.64 MiB\n",
            ".........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|end_of_text|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://b855875c87a678338f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "{'text': 'List 5 animals .', 'files': []}\n",
            "I 'I'\n",
            "'d \"'d\"\n",
            " be ' be'\n",
            " happy ' happy'\n",
            " to ' to'\n",
            " help ' help'\n",
            " you ' you'\n",
            " with ' with'\n",
            " that ' that'\n",
            "! '!'\n",
            " Here ' Here'\n",
            " are ' are'\n",
            " five ' five'\n",
            " animals ' animals'\n",
            ":\n",
            "\n",
            " ':\\n\\n'\n",
            "1 '1'\n",
            ". '.'\n",
            " Lion ' Lion'\n",
            "\n",
            " '\\n'\n",
            "2 '2'\n",
            ". '.'\n",
            " Elephant ' Elephant'\n",
            "\n",
            " '\\n'\n",
            "3 '3'\n",
            ". '.'\n",
            " Gor ' Gor'\n",
            "illa 'illa'\n",
            "\n",
            " '\\n'\n",
            "4 '4'\n",
            ". '.'\n",
            " Kang ' Kang'\n",
            "aroo 'aroo'\n",
            "\n",
            " '\\n'\n",
            "5 '5'\n",
            ". '.'\n",
            " Panda ' Panda'\n",
            "\n",
            "\n",
            " '\\n\\n'\n",
            "Let 'Let'\n",
            " me ' me'\n",
            " know ' know'\n",
            " if ' if'\n",
            " there ' there'\n",
            "'s \"'s\"\n",
            " anything ' anything'\n",
            " else ' else'\n",
            " I ' I'\n",
            " can ' can'\n",
            " assist ' assist'\n",
            " you ' you'\n",
            " with ' with'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    2572.40 ms\n",
            "llama_print_timings:      sample time =      30.58 ms /    51 runs   (    0.60 ms per token,  1667.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2572.25 ms /    26 tokens (   98.93 ms per token,    10.11 tokens per second)\n",
            "llama_print_timings:        eval time =   12573.99 ms /    50 runs   (  251.48 ms per token,     3.98 tokens per second)\n",
            "llama_print_timings:       total time =   16011.64 ms /    76 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ''\n",
            " ''\n",
            "{'text': '使用Javascript ,CSS寫一個精美的月歴，要正确顯示今天的位置，  所有內容 在同一個Html 中。', 'files': []}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下 '以下'\n",
            "是一 '是一'\n",
            "個 '個'\n",
            "簡 '簡'\n",
            "單 '單'\n",
            "的 '的'\n",
            "HTML 'HTML'\n",
            "、 '、'\n",
            "CSS 'CSS'\n",
            "和 '和'\n",
            "JavaScript 'JavaScript'\n",
            "範 '範'\n",
            "例 '例'\n",
            "， '，'\n",
            "展示 '展示'\n",
            "了 '了'\n",
            "如何 '如何'\n",
            "在 '在'\n",
            "同 '同'\n",
            "一個 '一個'\n",
            "HTML 'HTML'\n",
            "中 '中'\n",
            "創 '創'\n",
            "建 '建'\n",
            "一個 '一個'\n",
            "精 '精'\n",
            "美 '美'\n",
            "的 '的'\n",
            "月 '月'\n",
            "歷 '歷'\n",
            "，並 '，並'\n",
            "正 '正'\n",
            "確 '確'\n",
            "地 '地'\n",
            "顯 '顯'\n",
            "示 '示'\n",
            "今天 '今天'\n",
            "的 '的'\n",
            "位置 '位置'\n",
            "：\n",
            " '：\\n'\n",
            "``` '```'\n",
            "html 'html'\n",
            "\n",
            " '\\n'\n",
            "<! '<!'\n",
            "DOCTYPE 'DOCTYPE'\n",
            " html ' html'\n",
            ">\n",
            " '>\\n'\n",
            "<html '<html'\n",
            " lang ' lang'\n",
            "=\" '=\"'\n",
            "zh 'zh'\n",
            "-T '-T'\n",
            "W 'W'\n",
            "\">\n",
            " '\">\\n'\n",
            "<head '<head'\n",
            ">\n",
            " '>\\n'\n",
            "    '   '\n",
            " < ' <'\n",
            "meta 'meta'\n",
            " charset ' charset'\n",
            "=\" '=\"'\n",
            "UTF 'UTF'\n",
            "- '-'\n",
            "8 '8'\n",
            "\">\n",
            " '\">\\n'\n",
            "    '   '\n",
            " < ' <'\n",
            "meta 'meta'\n",
            " name ' name'\n",
            "=\" '=\"'\n",
            "viewport 'viewport'\n",
            "\" '\"'\n",
            " content ' content'\n",
            "=\" '=\"'\n",
            "width 'width'\n",
            "=device '=device'\n",
            "-width '-width'\n",
            ", ','\n",
            " initial ' initial'\n",
            "-scale '-scale'\n",
            "= '='\n",
            "1 '1'\n",
            ". '.'\n",
            "0 '0'\n",
            "\">\n",
            " '\">\\n'\n",
            "    '   '\n",
            " < ' <'\n",
            "title 'title'\n",
            "> '>'\n",
            "Month 'Month'\n",
            " Calendar ' Calendar'\n",
            "</ '</'\n",
            "title 'title'\n",
            ">\n",
            " '>\\n'\n",
            "    '   '\n",
            " < ' <'\n",
            "link 'link'\n",
            " rel ' rel'\n",
            "=\" '=\"'\n",
            "stylesheet 'stylesheet'\n",
            "\" '\"'\n",
            " href ' href'\n",
            "=\" '=\"'\n",
            "styles 'styles'\n",
            ".css '.css'\n",
            "\">\n",
            " '\">\\n'\n",
            "</ '</'\n",
            "head 'head'\n",
            ">\n",
            " '>\\n'\n",
            "<body '<body'\n",
            ">\n",
            " '>\\n'\n",
            "    '   '\n",
            " < ' <'\n",
            "div 'div'\n",
            " id ' id'\n",
            "=\" '=\"'\n",
            "calendar 'calendar'\n",
            "\"></ '\"></'\n",
            "div 'div'\n",
            ">\n",
            " '>\\n'\n",
            "    '   '\n",
            " < ' <'\n",
            "script 'script'\n",
            " src ' src'\n",
            "=\" '=\"'\n",
            "script 'script'\n",
            ".js '.js'\n",
            "\"></ '\"></'\n",
            "script 'script'\n",
            ">\n",
            " '>\\n'\n",
            "</ '</'\n",
            "body 'body'\n",
            ">\n",
            " '>\\n'\n",
            "</ '</'\n",
            "html 'html'\n",
            ">\n",
            " '>\\n'\n",
            "`` '``'\n",
            "`\n",
            "\n",
            " '`\\n\\n'\n",
            "``` '```'\n",
            "css 'css'\n",
            "\n",
            " '\\n'\n",
            "/* '/*'\n",
            " styles ' styles'\n",
            ".css '.css'\n",
            " */\n",
            " ' */\\n'\n",
            "# '#'\n",
            "calendar 'calendar'\n",
            " {\n",
            " ' {\\n'\n",
            "    '   '\n",
            " width ' width'\n",
            ": ':'\n",
            "  ' '\n",
            "300 '300'\n",
            "px 'px'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " height ' height'\n",
            ": ':'\n",
            "  ' '\n",
            "200 '200'\n",
            "px 'px'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " border ' border'\n",
            ": ':'\n",
            "  ' '\n",
            "1 '1'\n",
            "px 'px'\n",
            " solid ' solid'\n",
            " # ' #'\n",
            "ccc 'ccc'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " padding ' padding'\n",
            ": ':'\n",
            "  ' '\n",
            "10 '10'\n",
            "px 'px'\n",
            ";\n",
            " ';\\n'\n",
            "}\n",
            "\n",
            " '}\\n\\n'\n",
            ".day '.day'\n",
            " {\n",
            " ' {\\n'\n",
            "    '   '\n",
            " display ' display'\n",
            ": ':'\n",
            " inline ' inline'\n",
            "-block '-block'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " width ' width'\n",
            ": ':'\n",
            " calc ' calc'\n",
            "( '('\n",
            "100 '100'\n",
            "% '%'\n",
            " / ' /'\n",
            "  ' '\n",
            "7 '7'\n",
            ");\n",
            " ');\\n'\n",
            "    '   '\n",
            " text ' text'\n",
            "-align '-align'\n",
            ": ':'\n",
            " center ' center'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " font ' font'\n",
            "-size '-size'\n",
            ": ':'\n",
            "  ' '\n",
            "14 '14'\n",
            "px 'px'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " color ' color'\n",
            ": ':'\n",
            " # ' #'\n",
            "666 '666'\n",
            ";\n",
            " ';\\n'\n",
            "}\n",
            "\n",
            " '}\\n\\n'\n",
            ".today '.today'\n",
            " {\n",
            " ' {\\n'\n",
            "    '   '\n",
            " background ' background'\n",
            "-color '-color'\n",
            ": ':'\n",
            " # ' #'\n",
            "f 'f'\n",
            "0 '0'\n",
            "f 'f'\n",
            "0 '0'\n",
            "f 'f'\n",
            "0 '0'\n",
            ";\n",
            " ';\\n'\n",
            "    '   '\n",
            " border ' border'\n",
            "-bottom '-bottom'\n",
            ": ':'\n",
            "  ' '\n",
            "1 '1'\n",
            "px 'px'\n",
            " solid ' solid'\n",
            " # ' #'\n",
            "ccc 'ccc'\n",
            ";\n",
            " ';\\n'\n",
            "}\n",
            "\n",
            " '}\\n\\n'\n",
            " weekends ' weekends'\n",
            " {\n",
            " ' {\\n'\n",
            "    '   '\n",
            " background ' background'\n",
            "-color '-color'\n",
            ": ':'\n",
            " # ' #'\n",
            "eee 'eee'\n",
            ";\n",
            " ';\\n'\n",
            "}\n",
            " '}\\n'\n",
            "`` '``'\n",
            "`\n",
            "\n",
            " '`\\n\\n'\n",
            "``` '```'\n",
            "javascript 'javascript'\n",
            "\n",
            " '\\n'\n",
            "// '//'\n",
            " script ' script'\n",
            ".js '.js'\n",
            "\n",
            " '\\n'\n",
            "const 'const'\n",
            " calendar ' calendar'\n",
            "Container 'Container'\n",
            " = ' ='\n",
            " document ' document'\n",
            ".getElementById '.getElementById'\n",
            "(' \"('\"\n",
            "calendar 'calendar'\n",
            "');\n",
            " \"');\\n\"\n",
            "let 'let'\n",
            " today ' today'\n",
            "Date 'Date'\n",
            " = ' ='\n",
            " new ' new'\n",
            " Date ' Date'\n",
            "();\n",
            " '();\\n'\n",
            "let 'let'\n",
            " current ' current'\n",
            "Month 'Month'\n",
            " = ' ='\n",
            " today ' today'\n",
            "Date 'Date'\n",
            ".getMonth '.getMonth'\n",
            "();\n",
            " '();\\n'\n",
            "let 'let'\n",
            " current ' current'\n",
            "Year 'Year'\n",
            " = ' ='\n",
            " today ' today'\n",
            "Date 'Date'\n",
            ".getFullYear '.getFullYear'\n",
            "();\n",
            "\n",
            " '();\\n\\n'\n",
            "function 'function'\n",
            " create ' create'\n",
            "Calendar 'Calendar'\n",
            "(year '(year'\n",
            ", ','\n",
            " month ' month'\n",
            ") ')'\n",
            " {\n",
            " ' {\\n'\n",
            "    '   '\n",
            " const ' const'\n",
            " first ' first'\n",
            "Day 'Day'\n",
            "OfMonth 'OfMonth'\n",
            " = ' ='\n",
            " new ' new'\n",
            " Date ' Date'\n",
            "(year '(year'\n",
            ", ','\n",
            " month ' month'\n",
            ", ','\n",
            "  ' '\n",
            "1 '1'\n",
            ");\n",
            " ');\\n'\n",
            "    '   '\n",
            " const ' const'\n",
            " last ' last'\n",
            "Day 'Day'\n",
            "OfMonth 'OfMonth'\n",
            " = ' ='\n",
            " new ' new'\n",
            " Date ' Date'\n",
            "(year '(year'\n",
            ", ','\n",
            " month ' month'\n",
            " + ' +'\n",
            "  ' '\n",
            "1 '1'\n",
            ", ','\n",
            "  ' '\n",
            "0 '0'\n",
            ");\n",
            " ');\\n'\n",
            "    '   '\n",
            " const ' const'\n",
            " days ' days'\n",
            "In 'In'\n",
            "Month 'Month'\n",
            " = ' ='\n",
            " last ' last'\n",
            "Day 'Day'\n",
            "OfMonth 'OfMonth'\n",
            ".getDate '.getDate'\n",
            "();\n",
            " '();\\n'\n",
            "    '   '\n",
            " const ' const'\n",
            " day ' day'\n",
            "OfWeek 'OfWeek'\n",
            "First 'First'\n",
            "Day 'Day'\n",
            " = ' ='\n",
            " first ' first'\n",
            "Day 'Day'\n",
            "OfMonth 'OfMonth'\n",
            ".getDay '.getDay'\n",
            "();\n",
            "\n",
            " '();\\n\\n'\n",
            "    '   '\n",
            " calendar ' calendar'\n",
            "Container 'Container'\n",
            ".innerHTML '.innerHTML'\n",
            " = ' ='\n",
            " '';\n",
            "\n",
            " \" '';\\n\\n\"\n",
            "    '   '\n",
            " for ' for'\n",
            " ( ' ('\n",
            "let 'let'\n",
            " i ' i'\n",
            " = ' ='\n",
            "  ' '\n",
            "0 '0'\n",
            "; ';'\n",
            " i ' i'\n",
            " < ' <'\n",
            " day ' day'\n",
            "OfWeek 'OfWeek'\n",
            "First 'First'\n",
            "Day 'Day'\n",
            "; ';'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "from llama_cpp import Llama,LlamaRAMCache,LlamaDiskCache,LlamaTokenizer,LlamaState\n",
        "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
        "from llama_cpp.llama_chat_format import Llava15ChatHandler\n",
        "import os,torch,json,shlex\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except :IN_COLAB=False\n",
        "Gbase=\"./generate/\"\n",
        "cache_dir=\"./hf/\"\n",
        "\n",
        "import torch\n",
        "from psutil import cpu_count\n",
        "import platform\n",
        "if torch.cuda.is_available():\n",
        "    n_gpu_layers=-1\n",
        "    n_threads=cpu_count()\n",
        "else :\n",
        "    n_gpu_layers=0\n",
        "    n_threads=cpu_count()\n",
        "\n",
        "if platform.machine()=='a0arch64' and not IN_COLAB:n_threads=4\n",
        "\n",
        "\n",
        "description =\"\"\"<div style=\"font-family: Arial, sans-serif; padding: 20px;\">\n",
        "\n",
        "This showcases the use of llama-cpp-python to load different versions of 4-bit and 8-bit models, as well as a simple chat interface created by Gradio. It can run on small computers or even mobile devices and produce satisfactory results. You are free to modify this code. I will continue to update these tools, first unlocking more possibilities to allow this tool to handle most common content, and then combining all of these possibilities with automation elements. Ultimately, it will evolve into a powerful tool that solves real-world problems effectively.\n",
        "\n",
        "這裡展示了使用llama-cpp-python來加載各種4位和8位模型的不同版本，以及由Gradio創建的簡單聊天界面。它可以在小型電腦甚至手機上運行，並能產生令人滿意的結果。您可以自由修改這段代碼。我將繼續更新這些工具，首先解鎖更多可能性，使此工具能夠處理大多數常見內容，然後將所有這些可能性與自動化元素結合在一起。最終，它將發展成一個有效解決現實問題的工具。\n",
        "  <h2>🔗My social media links❤️</h2>\n",
        "\n",
        "Follow <a href=\"https://www.facebook.com/braiml\"  ytarget=\"_blank\">🐍Brian's Page </a> if you want I share more tools .<br>\n",
        "Follow<a href=\"https://www.facebook.com/charactersAI\" target=\"_blank\">❤️Characters AI</a>\n",
        " if you want more  videos .\n",
        "<br>\n",
        "  <a href=\"https://www.facebook.com/brian.pyai\" target=\"_blank\"> 📘facebook.com/brian.pyai</a>\n",
        " <br>\n",
        "  <a href=\"https://www.facebook.com/braiml\" target=\"_blank\">🐍Brian's Page </a>\n",
        "<br>\n",
        "\n",
        "  <a href=\"https://www.facebook.com/lovelyai999\" target=\"_blank\">🥰AI Hot Shorts </a>\n",
        "\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "\"\"\"\n",
        "import base64\n",
        "import io\n",
        "def image_to_base64_data_uri(file_path):\n",
        "    i=Image.open(file_path)\n",
        "    i.resize((256,256))\n",
        "    img_byte_arr = io.BytesIO()\n",
        "    i.save(img_byte_arr, format='PNG')\n",
        "    base64_data = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
        "    return f\"data:image/png;base64,{base64_data}\"\n",
        "\n",
        "modelsPath=\"./\"\n",
        "\n",
        "modelsPaths= [\"mradermacher/llava-v1.6-mistral-7b-GGUF/llava-v1.6-mistral-7b.Q4_K_M.gguf\",\"SanctumAI/Phi-3-mini-4k-instruct-GGUF/phi-3-mini-4k-instruct.Q4_K_M.gguf\",\"mradermacher/Llama3-8B-DPO-uncensored-GGUF/Llama3-8B-DPO-uncensored.Q8_0.gguf\",\"mradermacher/Llama3-Inst-8B-DPO-Ultrafeedback-GGUF/Llama3-Inst-8B-DPO-Ultrafeedback.Q8_0.gguf\",\"DavidAU/Llama3-8B-OpenHermes-DPO-Q8_0-GGUF/llama3-8b-openhermes-dpo.Q8_0.gguf\",\"LoneStriker/OrpoLlama-3-8B-GGUF/OrpoLlama-3-8B-Q8_0.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q4_K_M.gguf\",\"SanctumAI/Phi-3-mini-4k-instruct-GGUF/phi-3-mini-4k-instruct.Q8_0.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q8_0.gguf\",\"FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q4_K_M.gguf\",\"seyf1elislam/llama-3-neural-chat-v1-8b-GGUF/llama-3-neural-chat-v1-8b.Q4_K_M.gguf\",\"seyf1elislam/llama-3-neural-chat-v1-8b-GGUF/llama-3-neural-chat-v1-8b.Q8_0.gguf\",\"Quant-Cartel/Llama-3-8B-Instruct-DADA-iMat-GGUF/Llama-3-8B-Instruct-DADA-iMat-IQ4_XS.gguf\",\"Quant-Cartel/Llama-3-8B-Instruct-DADA-iMat-GGUF/Llama-3-8B-Instruct-DADA-iMat-Q8_0.gguf\",\"3thn/dolphin-2.9-llama3-8b-GGUF/dolphin-2.9-llama3-8b.Q4_K_M.gguf\",\"3thn/dolphin-2.9-llama3-8b-GGUF/dolphin-2.9-llama3-8b.Q8_0.gguf\",\"PawanKrd/Meta-Llama-3-70B-Instruct-GGUF/llama-3-70b-instruct.Q3_K_M.gguf\",\"twodgirl/zephyr-beta-wizardLM-2-merge-7B-Q6_K-GGUF/zephyr-beta-wizardlm-2-merge-7b.Q6_K.gguf\",\"MaziyarPanahi/WizardLM-2-8x22B-GGUF/WizardLM-2-8x22B.IQ1_M.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q2_K.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf\",\"mradermacher/Starling-LM-alpha-8x7B-MoE-GGUF/Starling-LM-alpha-8x7B-MoE.Q4_K_M.gguf\",\"MaziyarPanahi/Qwen1.5-8x7b-v0.1-GGUF/Qwen1.5-8x7b-v0.1.Q4_K_M.gguf\",\"TheBloke/firefly-mixtral-8x7b-GGUF/firefly-mixtral-8x7b.Q4_K_M.gguf\",\"TheBloke/openbuddy-mixtral-8x7b-v15.1-GGUF/openbuddy-mixtral-8x7b-v15.1.Q4_K_M.gguf\",\"Quant-Cartel/Cerebrum-1.0-8x7b-iMat-GGUF/Cerebrum-1.0-8x7b-iMat-Q4_K_M.gguf\",\"MaziyarPanahi/Experiment26-7B-GGUF/Experiment26-7B.Q4_K_M.gguf\",\"dagbs/dolphin-2.8-experiment26-7b-preview-GGUF/dolphin-2.8-experiment26-7b.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q4_K_M-GGUF/gemma-1.1-2b-it.Q4_K_M.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q4_k_m.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/gemma-1.1-7b-it.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/gemma-1.1-2b-it.Q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q8_0-GGUF/gemma-1.1-7b-it.Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/gemma-2b-GGUF/gemma-2b-Q4_K_M.gguf\",\"LoneStriker/gemma-2b-it-GGUF/gemma-2b-it-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-GGUF/gemma-7b-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-it-GGUF/gemma-7b-it-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/OrcaGemma-2B-GGUF/OrcaGemma-2B-Q4_K_M.gguf\",\"rombodawg/EveryoneLLM-7b-Gemma-Base-GGUF/EveryoneLLM-7b-Gemma-Base-q6_k.gguf\",\"LoneStriker/openbuddy-gemma-7b-v19.1-4k-GGUF/openbuddy-gemma-7b-v19.1-4k-Q4_K_M.gguf\",\"LoneStriker/Gemmalpaca-7B-GGUF/Gemmalpaca-7B-Q4_K_M.gguf\", \"LoneStriker/zephyr-7b-gemma-v0.1-GGUF/zephyr-7b-gemma-v0.1-Q4_K_M.gguf\" ,\"Lewdiculous/firefly-gemma-7b-GGUF-IQ-Imatrix/firefly-gemma-7b-Q4_K_S-imatrix.gguf\"]\n",
        "\n",
        "\n",
        "\n",
        "model_id=\"SanctumAI/Phi-3-mini-4k-instruct-GGUF/phi-3-mini-4k-instruct.Q8_0.gguf\" #@param      [\"mradermacher/llava-v1.6-mistral-7b-GGUF/llava-v1.6-mistral-7b.Q4_K_M.gguf\",\"SanctumAI/Phi-3-mini-4k-instruct-GGUF/phi-3-mini-4k-instruct.Q4_K_M.gguf\",\"mradermacher/Llama3-8B-DPO-uncensored-GGUF/Llama3-8B-DPO-uncensored.Q8_0.gguf\",\"mradermacher/Llama3-Inst-8B-DPO-Ultrafeedback-GGUF/Llama3-Inst-8B-DPO-Ultrafeedback.Q8_0.gguf\",\"DavidAU/Llama3-8B-OpenHermes-DPO-Q8_0-GGUF/llama3-8b-openhermes-dpo.Q8_0.gguf\",\"LoneStriker/OrpoLlama-3-8B-GGUF/OrpoLlama-3-8B-Q8_0.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q4_K_M.gguf\",\"SanctumAI/Phi-3-mini-4k-instruct-GGUF/phi-3-mini-4k-instruct.Q8_0.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q8_0.gguf\",\"FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q4_K_M.gguf\",\"seyf1elislam/llama-3-neural-chat-v1-8b-GGUF/llama-3-neural-chat-v1-8b.Q4_K_M.gguf\",\"seyf1elislam/llama-3-neural-chat-v1-8b-GGUF/llama-3-neural-chat-v1-8b.Q8_0.gguf\",\"Quant-Cartel/Llama-3-8B-Instruct-DADA-iMat-GGUF/Llama-3-8B-Instruct-DADA-iMat-IQ4_XS.gguf\",\"Quant-Cartel/Llama-3-8B-Instruct-DADA-iMat-GGUF/Llama-3-8B-Instruct-DADA-iMat-Q8_0.gguf\",\"3thn/dolphin-2.9-llama3-8b-GGUF/dolphin-2.9-llama3-8b.Q4_K_M.gguf\",\"3thn/dolphin-2.9-llama3-8b-GGUF/dolphin-2.9-llama3-8b.Q8_0.gguf\",\"PawanKrd/Meta-Llama-3-70B-Instruct-GGUF/llama-3-70b-instruct.Q3_K_M.gguf\",\"twodgirl/zephyr-beta-wizardLM-2-merge-7B-Q6_K-GGUF/zephyr-beta-wizardlm-2-merge-7b.Q6_K.gguf\",\"MaziyarPanahi/WizardLM-2-8x22B-GGUF/WizardLM-2-8x22B.IQ1_M.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q2_K.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf\",\"mradermacher/Starling-LM-alpha-8x7B-MoE-GGUF/Starling-LM-alpha-8x7B-MoE.Q4_K_M.gguf\",\"MaziyarPanahi/Qwen1.5-8x7b-v0.1-GGUF/Qwen1.5-8x7b-v0.1.Q4_K_M.gguf\",\"TheBloke/firefly-mixtral-8x7b-GGUF/firefly-mixtral-8x7b.Q4_K_M.gguf\",\"TheBloke/openbuddy-mixtral-8x7b-v15.1-GGUF/openbuddy-mixtral-8x7b-v15.1.Q4_K_M.gguf\",\"Quant-Cartel/Cerebrum-1.0-8x7b-iMat-GGUF/Cerebrum-1.0-8x7b-iMat-Q4_K_M.gguf\",\"MaziyarPanahi/Experiment26-7B-GGUF/Experiment26-7B.Q4_K_M.gguf\",\"dagbs/dolphin-2.8-experiment26-7b-preview-GGUF/dolphin-2.8-experiment26-7b.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q4_K_M-GGUF/gemma-1.1-2b-it.Q4_K_M.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q4_k_m.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/gemma-1.1-7b-it.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/gemma-1.1-2b-it.Q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q8_0-GGUF/gemma-1.1-7b-it.Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/gemma-2b-GGUF/gemma-2b-Q4_K_M.gguf\",\"LoneStriker/gemma-2b-it-GGUF/gemma-2b-it-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-GGUF/gemma-7b-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-it-GGUF/gemma-7b-it-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/OrcaGemma-2B-GGUF/OrcaGemma-2B-Q4_K_M.gguf\",\"rombodawg/EveryoneLLM-7b-Gemma-Base-GGUF/EveryoneLLM-7b-Gemma-Base-q6_k.gguf\",\"LoneStriker/openbuddy-gemma-7b-v19.1-4k-GGUF/openbuddy-gemma-7b-v19.1-4k-Q4_K_M.gguf\",\"LoneStriker/Gemmalpaca-7B-GGUF/Gemmalpaca-7B-Q4_K_M.gguf\", \"LoneStriker/zephyr-7b-gemma-v0.1-GGUF/zephyr-7b-gemma-v0.1-Q4_K_M.gguf\" ,\"Lewdiculous/firefly-gemma-7b-GGUF-IQ-Imatrix/firefly-gemma-7b-Q4_K_S-imatrix.gguf\"]\n",
        "def selectPath(paths=modelsPaths):\n",
        "    #return \"gemma-openchat7v-model-Q4_K_M.gguf\"\n",
        "    ls=paths\n",
        "    #c='termux-dialog radio -v \"%s\" -t \"Select model\"' % \",\".join(ls)\n",
        "    #v=eval(os.popen(c).read())[\"text\"]\n",
        "    for i,t in enumerate(ls):print (\"%s) %s\" % (i ,t) )\n",
        "    print (\"input the number :\")\n",
        "    v=ls[int(input())]\n",
        "\n",
        "    return v\n",
        "if not IN_COLAB:\n",
        "    model_id=selectPath()\n",
        "def downHgFile(url,targetDir=modelsPath):\n",
        "    fileName=Path (model_id ).name\n",
        "    repo =model_id[:-len(fileName)-1]\n",
        "    print(fileName,repo)\n",
        "    fileExists=os.path.exists(os.path.join(targetDir,fileName))\n",
        "\n",
        "    print(repo,fileName,fileExists )\n",
        "    if not fileExists:\n",
        "        print (\"Downloading file :\")\n",
        "        hf_hub_download(repo ,filename=fileName,local_dir=targetDir,local_dir_use_symlinks=False)\n",
        "max_tokens=4096 # @param {type:\"integer\",min:10, max:8192}\n",
        "n_ctx=4096 # @param {type:\"integer\",min:10, max:8192}\n",
        "downHgFile(model_id )\n",
        "#hf_hub_download(\"lovelyai999/temp\" ,filename=\"mistral_7b_mmproj-v1_5_Q4_1.gguf\",local_dir=\"./\",local_dir_use_symlinks=False)\n",
        "modelPath=os.path.join(modelsPath,Path(model_id).name)\n",
        "if (\"8x\" in model_id or \"70b\" in model_id) and torch.cuda.is_available():\n",
        "    n_gpu_layers=16\n",
        "    if  \"70b\" in model_id:\n",
        "        n_gpu_layers=24\n",
        "    n_threads=4\n",
        "if model_id==\"mradermacher/llava-v1.6-mistral-7b-GGUF/llava-v1.6-mistral-7b.Q4_K_M.gguf\":\n",
        "    if not os.path.exists(\"mistral_7b_mmproj-v1_5_Q4_1.gguf\"):hf_hub_download(\"lovelyai999/temp\" ,filename=\"mistral_7b_mmproj-v1_5_Q4_1.gguf\",local_dir=\"./\",local_dir_use_symlinks=False)\n",
        "    chat_handler = Llava15ChatHandler(clip_model_path=\"mistral_7b_mmproj-v1_5_Q4_1.gguf\",verbose=True)\n",
        "    model = Llama(modelPath,n_gpu_layers=n_gpu_layers,n_threads=n_threads,max_tokens=4096,logits_all=True,n_ctx=n_ctx,chat_handler=chat_handler)\n",
        "else:\n",
        "    model = Llama(modelPath,n_gpu_layers=n_gpu_layers,n_threads=n_threads,max_tokens=4096,logits_all=True,n_ctx=n_ctx)\n",
        "#draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) )\n",
        "\n",
        "tokenizer = model.tokenize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "from threading import Thread\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "partial_message = \"\"\n",
        "\n",
        "def predict(message, history,top_k=300, top_p=0.95, temp=0.85, repeat_penalty=1.1,max_tokens=max_tokens):\n",
        "    global partial_message,model_id\n",
        "    try :messageT=message[\"text\"]\n",
        "    except :messageT=message\n",
        "    global model_id\n",
        "    if \"phi-3-\" in model_id:\n",
        "        prompt=f\"<|user|>{messageT}<|end|><|assistant|>\"\n",
        "    else:\n",
        "        prompt=f\"\"\"### System:\n",
        "You are a professional private assistant.\n",
        "### User:\n",
        "{messageT}\n",
        "###  Response:\n",
        "\n",
        "\"\"\"\n",
        "    prompt_=f\"\"\"\n",
        "###Below is an instruction that describes my question or task.\n",
        "Write a response that appropriately completes the request:\n",
        "{messageT}\n",
        "### Response :\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    print (message)\n",
        "    #model_inputs = tokenizer(prompt.encode(\"utf-8\"))\n",
        "    #generate_kwargs = dict(top_k=top_k, top_p=top_p, temp=temp, repeat_penalty=repeat_penalty)\n",
        "    stop= [\"<|end|>\" ,\"<|end_of_text|>\", \"<|im_end|>\"  ]\n",
        "    if model_id in [\"FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q4_K_M.gguf\",\"l3utterfly/llama-3-8b-Instruct-gguf/llama-3-8b-Instruct-Q4_K.gguf\",\"l3utterfly/llama-3-8b-Instruct-gguf/llama-3-8b-Instruct-Q8_0.gguf\",\"PawanKrd/Meta-Llama-3-70B-Instruct-GGUFFaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-70b-instruct.Q2_K.gguf\",\"PawanKrd/Meta-Llama-3-70B-Instruct-GGUF/llama-3-70b-instruct.Q3_K_M.gguf\", \"LoneStriker/OrpoLlama-3-8B-GGUF/OrpoLlama-3-8B-Q8_0.gguf\" ,\"mradermacher/Llama3-8B-DPO-uncensored-GGUF/Llama3-8B-DPO-uncensored.Q8_0.gguf\",\"mradermacher/Llama3-Inst-8B-DPO-Ultrafeedback-GGUF/Llama3-Inst-8B-DPO-Ultrafeedback.Q8_0.gguf\",\"DavidAU/Llama3-8B-OpenHermes-DPO-Q8_0-GGUF/llama3-8b-openhermes-dpo.Q8_0.gguf\" ]:\n",
        "        stop=[\"### Below\",\".\\n\\n\",\"assistant\\n\\n\",\"!\\n\\n\",\"<|end|>\" ,\"<|end_of_text|>\" ,\"<|im_end|>\",\"System:\"   ]\n",
        "    generate_kwargs=dict (suffix=None, max_tokens=max_tokens, temperature=temp, top_p=top_p, min_p=0.05, typical_p=1.0, logprobs=None, echo=False, stop=stop, frequency_penalty=0,presence_penalty=0.0, repeat_penalty=1.1, top_k=top_k, stream=True , seed=None, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None, stopping_criteria=None, logits_processor=None, grammar=None, logit_bias=None)\n",
        "\n",
        "\n",
        "    partial_message = \"\"\n",
        "    if isinstance(message,dict) and message[\"files\"] and message[\"files\"][0][\"path\"] and message[\"files\"][0]['mime_type'].startswith(\"image\"):\n",
        "        messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\":message[\"files\"][0][\"path\"]}},\n",
        "                {\"type\" : \"text\", \"text\": messageT}\n",
        "            ]\n",
        "        }]\n",
        "        outputs =model.create_completion(json.dumps (messages),**generate_kwargs)\n",
        "    else:\n",
        "        outputs =model.create_completion(prompt ,**generate_kwargs)\n",
        "\n",
        "\n",
        "    for chunk in outputs:\n",
        "        #print (chunk )\n",
        "        content = chunk[\"choices\"][0][\"text\"]\n",
        "        #print(content,repr(content ))\n",
        "        if content:\n",
        "            partial_message+=content\n",
        "            yield partial_message\n",
        "    \"\"\"\n",
        "    for new_token in model.generate(model_inputs,**generate_kwargs):\n",
        "            #print (new_token)\n",
        "            if new_token in (213,7221,1):break\n",
        "            s=model.detokenize([new_token])\n",
        "            #print (s)\n",
        "            if isinstance(s,bytes):\n",
        "                try:partial_message += s.decode(\"utf-8\")\n",
        "                except :break\n",
        "            yield partial_message\n",
        "        \"\"\"\n",
        "\n",
        "chatbot = gr.Chatbot(likeable=True,show_copy_button=True)\n",
        "textbox = gr.Textbox (show_copy_button=True)\n",
        "\n",
        "user=\"brian\" #@param {type:\"string\"}\n",
        "password=\"pwd\" #@param {type:\"string\"}\n",
        "auth=(user,password)\n",
        "\n",
        "gr.ChatInterface(predict,chatbot=chatbot,stop_btn=\"Stop\" ,retry_btn=\"Retry\",concurrency_limit=1,description=description,\n",
        "additional_inputs=[gr.Number(value=100,minimum =0,maximum =1000,precision=0,show_label=True ,label=\"Topic K\"),\n",
        "gr.Number(value=0.85,minimum =0,maximum =1,precision=3,show_label=True ,label=\"Topic P\"),\n",
        "gr.Number(value=0.85,minimum =0,maximum =1,precision=3,show_label=True ,label=\"Temperature\"),gr.Number(value=1.1,minimum =1,maximum =3,precision=3,show_label=True ,label=\"Repeat penalty\") ,gr.Number(value=4096,minimum =0,maximum =8192,precision=0,show_label=True ,label=\"max_tokens\") ],multimodal=True ).launch(debug=True,share=True ,inline=False,auth=auth )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOvO2Tycb873D33IazAgGxg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}