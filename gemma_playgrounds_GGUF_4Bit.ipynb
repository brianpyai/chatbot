{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianpyai/chatbot/blob/main/gemma_playgrounds_GGUF_4Bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-czlAOa2dN4g",
        "outputId": "dc7726c8-707e-4ee1-d493-d9b1f5e8e41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio==4.22.0 in /usr/local/lib/python3.10/dist-packages (4.22.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.110.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.13.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.13.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.6.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.3.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (4.11.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.22.0) (0.29.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.13.0->gradio==4.22.0) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.13.0->gradio==4.22.0) (11.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.22.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.22.0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.22.0) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.22.0) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.22.0) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.22.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.22.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.22.0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.22.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.22.0) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.22.0) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.22.0) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.22.0) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.22.0) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==4.22.0) (0.37.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.22.0) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.22.0) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.22.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.22.0) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio==4.22.0) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.22.0) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gradio==4.22.0 transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ8HCwYutHId",
        "outputId": "07db411e-540a-4ad2-a072-769f2bbb466a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#@title Built llama-cpp-python with BLAS or CUDA automatically, wait for few minutes .\n",
        "import os,torch\n",
        "\n",
        "\n",
        "if torch.cuda.is_available ():\n",
        "    cmd='CMAKE_ARGS=\"%s\" pip install llama-cpp-python' % \"-DLLAMA_CUBLAS=on\"\n",
        "else:\n",
        "    cmd='CMAKE_ARGS=\"%s\" pip install llama-cpp-python' % \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"\n",
        "\n",
        "os.system (cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b4117347585b40b4b80ee67376475b82"
          ]
        },
        "id": "XOxIBjXDN2ee",
        "outputId": "49bfa15f-c7b3-42c0-8ab0-4fe0255583e4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WizardLM-2-7B.Q8_0.gguf MaziyarPanahi/WizardLM-2-7B-GGUF\n",
            "MaziyarPanahi/WizardLM-2-7B-GGUF WizardLM-2-7B.Q8_0.gguf False\n",
            "Downloading file :\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4117347585b40b4b80ee67376475b82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "WizardLM-2-7B.Q8_0.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./WizardLM-2-7B.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models--microsoft--WizardLM-2-7B\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = models--microsoft--WizardLM-2-7B\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models--microsoft--WizardLM-2-7B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '32000', 'llama.rope.dimension_count': '128'}\n",
            "Using fallback chat format: None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://18eed0d9d6733e5917.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "{'text': 'write a snap game with Javascript ', 'files': []}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "from llama_cpp import Llama,LlamaRAMCache,LlamaDiskCache,LlamaTokenizer,LlamaState\n",
        "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
        "from llama_cpp.llama_chat_format import Llava15ChatHandler\n",
        "import os,torch,json,shlex\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "IN_COLAB = False\n",
        "Gbase=\"./generate/\"\n",
        "cache_dir=\"./hf/\"\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    n_gpu_layers=-1\n",
        "else :\n",
        "    n_gpu_layers=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "description =\"\"\"<div style=\"font-family: Arial, sans-serif; padding: 20px;\">\n",
        "\n",
        "This showcases the use of llama-cpp-python to load different versions of 4-bit and 8-bit models, as well as a simple chat interface created by Gradio. It can run on small computers or even mobile devices and produce satisfactory results. You are free to modify this code. I will continue to update these tools, first unlocking more possibilities to allow this tool to handle most common content, and then combining all of these possibilities with automation elements. Ultimately, it will evolve into a powerful tool that solves real-world problems effectively.\n",
        "\n",
        "ÈÄôË£°Â±ïÁ§∫‰∫Ü‰ΩøÁî®llama-cpp-python‰æÜÂä†ËºâÂêÑÁ®Æ4‰ΩçÂíå8‰ΩçÊ®°ÂûãÁöÑ‰∏çÂêåÁâàÊú¨Ôºå‰ª•ÂèäÁî±GradioÂâµÂª∫ÁöÑÁ∞°ÂñÆËÅäÂ§©ÁïåÈù¢„ÄÇÂÆÉÂèØ‰ª•Âú®Â∞èÂûãÈõªËÖ¶ÁîöËá≥ÊâãÊ©ü‰∏äÈÅãË°åÔºå‰∏¶ËÉΩÁî¢Áîü‰ª§‰∫∫ÊªøÊÑèÁöÑÁµêÊûú„ÄÇÊÇ®ÂèØ‰ª•Ëá™Áî±‰øÆÊîπÈÄôÊÆµ‰ª£Á¢º„ÄÇÊàëÂ∞áÁπºÁ∫åÊõ¥Êñ∞ÈÄô‰∫õÂ∑•ÂÖ∑ÔºåÈ¶ñÂÖàËß£ÈéñÊõ¥Â§öÂèØËÉΩÊÄßÔºå‰ΩøÊ≠§Â∑•ÂÖ∑ËÉΩÂ§†ËôïÁêÜÂ§ßÂ§öÊï∏Â∏∏Ë¶ãÂÖßÂÆπÔºåÁÑ∂ÂæåÂ∞áÊâÄÊúâÈÄô‰∫õÂèØËÉΩÊÄßËàáËá™ÂãïÂåñÂÖÉÁ¥†ÁµêÂêàÂú®‰∏ÄËµ∑„ÄÇÊúÄÁµÇÔºåÂÆÉÂ∞áÁôºÂ±ïÊàê‰∏ÄÂÄãÊúâÊïàËß£Ê±∫ÁèæÂØ¶ÂïèÈ°åÁöÑÂ∑•ÂÖ∑„ÄÇ\n",
        "  <h2>üîóMy social media links‚ù§Ô∏è</h2>\n",
        "\n",
        "Follow <a href=\"https://www.facebook.com/braiml\"  ytarget=\"_blank\">üêçBrian's Page </a> if you want I share more tools .<br>\n",
        "Follow<a href=\"https://www.facebook.com/charactersAI\" target=\"_blank\">‚ù§Ô∏èCharacters AI</a>\n",
        " if you want more  videos .\n",
        "<br>\n",
        "  <a href=\"https://www.facebook.com/brian.pyai\" target=\"_blank\"> üìòfacebook.com/brian.pyai</a>\n",
        " <br>\n",
        "  <a href=\"https://www.facebook.com/braiml\" target=\"_blank\">üêçBrian's Page </a>\n",
        "<br>\n",
        "\n",
        "  <a href=\"https://www.facebook.com/lovelyai999\" target=\"_blank\">ü•∞AI Hot Shorts </a>\n",
        "\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "\"\"\"\n",
        "import base64\n",
        "import io\n",
        "def image_to_base64_data_uri(file_path):\n",
        "    i=Image.open(file_path)\n",
        "    i.resize((256,256))\n",
        "    img_byte_arr = io.BytesIO()\n",
        "    i.save(img_byte_arr, format='PNG')\n",
        "    base64_data = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
        "    return f\"data:image/png;base64,{base64_data}\"\n",
        "\n",
        "modelsPath=\"./\"\n",
        "\n",
        "modelsPaths=[\"mradermacher/llava-v1.6-mistral-7b-GGUF/llava-v1.6-mistral-7b.Q4_K_M.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q4_K_M.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q8_0.gguf\",\"MaziyarPanahi/WizardLM-2-8x22B-GGUF/WizardLM-2-8x22B.IQ1_M.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q2_K.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf\",\"LoneStriker/MixTAO-7Bx2-MoE-v8.1-GGUF/MixTAO-7Bx2-MoE-v8.1-Q4_K_M.gguf\",\"LoneStriker/MixTAO-7Bx2-MoE-v8.1-GGUF/MixTAO-7Bx2-MoE-v8.1-Q8_0.gguf\",\"mradermacher/Starling-LM-alpha-8x7B-MoE-GGUF/Starling-LM-alpha-8x7B-MoE.Q4_K_M.gguf\",\"MaziyarPanahi/Qwen1.5-8x7b-v0.1-GGUF/Qwen1.5-8x7b-v0.1.Q4_K_M.gguf\",\"TheBloke/firefly-mixtral-8x7b-GGUF/firefly-mixtral-8x7b.Q4_K_M.gguf\",\"TheBloke/openbuddy-mixtral-8x7b-v15.1-GGUF/openbuddy-mixtral-8x7b-v15.1.Q4_K_M.gguf\",\"Quant-Cartel/Cerebrum-1.0-8x7b-iMat-GGUF/Cerebrum-1.0-8x7b-iMat-Q4_K_M.gguf\",\"TheBloke/Llama-2-7B-LoRA-Assemble-GGUF/llama-2-7b-lora-assemble.Q4_K_M.gguf\",\"TheBloke/Llama-2-13B-LoRA-Assemble-GGUF/llama-2-13b-lora-assemble.Q4_K_M.gguf\",\"MaziyarPanahi/Experiment26-7B-GGUF/Experiment26-7B.Q4_K_M.gguf\",\"dagbs/dolphin-2.8-experiment26-7b-preview-GGUF/dolphin-2.8-experiment26-7b.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q4_K_M-GGUF/gemma-1.1-2b-it.Q4_K_M.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q4_k_m.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/gemma-1.1-7b-it.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/gemma-1.1-2b-it.Q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q8_0-GGUF/gemma-1.1-7b-it.Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/gemma-2b-GGUF/gemma-2b-Q4_K_M.gguf\",\"LoneStriker/gemma-2b-it-GGUF/gemma-2b-it-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-GGUF/gemma-7b-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-it-GGUF/gemma-7b-it-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/OrcaGemma-2B-GGUF/OrcaGemma-2B-Q4_K_M.gguf\",\"rombodawg/EveryoneLLM-7b-Gemma-Base-GGUF/EveryoneLLM-7b-Gemma-Base-q6_k.gguf\",\"LoneStriker/openbuddy-gemma-7b-v19.1-4k-GGUF/openbuddy-gemma-7b-v19.1-4k-Q4_K_M.gguf\",\"LoneStriker/Gemmalpaca-7B-GGUF/Gemmalpaca-7B-Q4_K_M.gguf\", \"LoneStriker/zephyr-7b-gemma-v0.1-GGUF/zephyr-7b-gemma-v0.1-Q4_K_M.gguf\" ,\"Lewdiculous/firefly-gemma-7b-GGUF-IQ-Imatrix/firefly-gemma-7b-Q4_K_S-imatrix.gguf\"]\n",
        "\n",
        "\n",
        "\n",
        "model_id=\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q4_k_m.gguf\" #@param [\"mradermacher/llava-v1.6-mistral-7b-GGUF/llava-v1.6-mistral-7b.Q4_K_M.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q4_K_M.gguf\",\"MaziyarPanahi/WizardLM-2-7B-GGUF/WizardLM-2-7B.Q8_0.gguf\",\"MaziyarPanahi/WizardLM-2-8x22B-GGUF/WizardLM-2-8x22B.IQ1_M.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q2_K.gguf\",\"TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf\",\"LoneStriker/MixTAO-7Bx2-MoE-v8.1-GGUF/MixTAO-7Bx2-MoE-v8.1-Q4_K_M.gguf\",\"LoneStriker/MixTAO-7Bx2-MoE-v8.1-GGUF/MixTAO-7Bx2-MoE-v8.1-Q8_0.gguf\",\"mradermacher/Starling-LM-alpha-8x7B-MoE-GGUF/Starling-LM-alpha-8x7B-MoE.Q4_K_M.gguf\",\"MaziyarPanahi/Qwen1.5-8x7b-v0.1-GGUF/Qwen1.5-8x7b-v0.1.Q4_K_M.gguf\",\"TheBloke/firefly-mixtral-8x7b-GGUF/firefly-mixtral-8x7b.Q4_K_M.gguf\",\"TheBloke/openbuddy-mixtral-8x7b-v15.1-GGUF/openbuddy-mixtral-8x7b-v15.1.Q4_K_M.gguf\",\"Quant-Cartel/Cerebrum-1.0-8x7b-iMat-GGUF/Cerebrum-1.0-8x7b-iMat-Q4_K_M.gguf\",\"TheBloke/Llama-2-7B-LoRA-Assemble-GGUF/llama-2-7b-lora-assemble.Q4_K_M.gguf\",\"TheBloke/Llama-2-13B-LoRA-Assemble-GGUF/llama-2-13b-lora-assemble.Q4_K_M.gguf\",\"MaziyarPanahi/Experiment26-7B-GGUF/Experiment26-7B.Q4_K_M.gguf\",\"dagbs/dolphin-2.8-experiment26-7b-preview-GGUF/dolphin-2.8-experiment26-7b.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q4_K_M-GGUF/gemma-1.1-2b-it.Q4_K_M.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q4_k_m.gguf\",\"chenhunghan/TAIDE-LX-7B-Chat-GGUF/taide-lx-7b-chat.q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/gemma-1.1-7b-it.Q4_K_M.gguf\",\"ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/gemma-1.1-2b-it.Q8_0.gguf\",\"ggml-org/gemma-1.1-7b-it-Q8_0-GGUF/gemma-1.1-7b-it.Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q8_0.gguf\",\"pi-null-mezon/openchat-3.5-0106-gemma-GGUF/ggml-model-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/gemma-2b-GGUF/gemma-2b-Q4_K_M.gguf\",\"LoneStriker/gemma-2b-it-GGUF/gemma-2b-it-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-GGUF/gemma-7b-Q4_K_M.gguf\",\"LoneStriker/gemma-7b-it-GGUF/gemma-7b-it-Q4_K_M.gguf\",\"mlabonne/Gemmalpaca-2B-GGUF/gemmalpaca-2b.Q4_K_M.gguf\",\"LoneStriker/OrcaGemma-2B-GGUF/OrcaGemma-2B-Q4_K_M.gguf\",\"rombodawg/EveryoneLLM-7b-Gemma-Base-GGUF/EveryoneLLM-7b-Gemma-Base-q6_k.gguf\",\"LoneStriker/openbuddy-gemma-7b-v19.1-4k-GGUF/openbuddy-gemma-7b-v19.1-4k-Q4_K_M.gguf\",\"LoneStriker/Gemmalpaca-7B-GGUF/Gemmalpaca-7B-Q4_K_M.gguf\", \"LoneStriker/zephyr-7b-gemma-v0.1-GGUF/zephyr-7b-gemma-v0.1-Q4_K_M.gguf\" ,\"Lewdiculous/firefly-gemma-7b-GGUF-IQ-Imatrix/firefly-gemma-7b-Q4_K_S-imatrix.gguf\"]\n",
        "\n",
        "def downHgFile(url,targetDir=modelsPath):\n",
        "    fileName=Path (model_id ).name\n",
        "    repo =model_id[:-len(fileName)-1]\n",
        "    print(fileName,repo)\n",
        "    fileExists=os.path.exists(os.path.join(targetDir,fileName))\n",
        "\n",
        "    print(repo,fileName,fileExists )\n",
        "    if not fileExists:\n",
        "        print (\"Downloading file :\")\n",
        "        hf_hub_download(repo ,filename=fileName,local_dir=targetDir,local_dir_use_symlinks=False)\n",
        "max_tokens=4096 # @param {type:\"integer\",min:10, max:8192}\n",
        "n_ctx=4096 # @param {type:\"integer\",min:10, max:8192}\n",
        "downHgFile(model_id )\n",
        "#hf_hub_download(\"lovelyai999/temp\" ,filename=\"mistral_7b_mmproj-v1_5_Q4_1.gguf\",local_dir=\"./\",local_dir_use_symlinks=False)\n",
        "modelPath=os.path.join(modelsPath,Path(model_id).name)\n",
        "if model_id==\"mradermacher/llava-v1.6-mistral-7b-GGUF/llava-v1.6-mistral-7b.Q4_K_M.gguf\":\n",
        "    if not os.path.exists(\"mistral_7b_mmproj-v1_5_Q4_1.gguf\"):hf_hub_download(\"lovelyai999/temp\" ,filename=\"mistral_7b_mmproj-v1_5_Q4_1.gguf\",local_dir=\"./\",local_dir_use_symlinks=False)\n",
        "    chat_handler = Llava15ChatHandler(clip_model_path=\"mistral_7b_mmproj-v1_5_Q4_1.gguf\",verbose=True)\n",
        "    model = Llama(modelPath,n_gpu_layers=n_gpu_layers,n_threads=4,max_tokens=4096,logits_all=True,n_ctx=n_ctx,chat_handler=chat_handler)\n",
        "else:\n",
        "    model = Llama(modelPath,n_gpu_layers=n_gpu_layers,n_threads=4,max_tokens=4096,logits_all=True,n_ctx=n_ctx)\n",
        "#draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) )\n",
        "\n",
        "tokenizer = model.tokenize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "from threading import Thread\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "partial_message = \"\"\n",
        "\n",
        "def predict(message, history,top_k=300, top_p=0.95, temp=0.85, repeat_penalty=1.1,max_tokens=max_tokens):\n",
        "    global partial_message\n",
        "    try :messageT=message[\"text\"]\n",
        "    except :messageT=message\n",
        "    prompt=f\"\"\"\n",
        "###Below is an instruction that describes my question or task.\n",
        "Write a response that appropriately completes the request:\n",
        "{messageT}\n",
        "### Response :\n",
        "\n",
        "\"\"\"\n",
        "    print (message)\n",
        "    #model_inputs = tokenizer(prompt.encode(\"utf-8\"))\n",
        "    #generate_kwargs = dict(top_k=top_k, top_p=top_p, temp=temp, repeat_penalty=repeat_penalty)\n",
        "    generate_kwargs=dict (suffix=None, max_tokens=max_tokens, temperature=temp, top_p=top_p, min_p=0.05, typical_p=1.0, logprobs=None, echo=False, stop=[], frequency_penalty=0.0, presence_penalty=0.0, repeat_penalty=1.1, top_k=top_k, stream=True , seed=None, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None, stopping_criteria=None, logits_processor=None, grammar=None, logit_bias=None)\n",
        "\n",
        "\n",
        "    partial_message = \"\"\n",
        "    if isinstance(message,dict) and message[\"files\"] and message[\"files\"][0][\"path\"] and message[\"files\"][0]['mime_type'].startswith(\"image\"):\n",
        "        messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\":message[\"files\"][0][\"path\"]}},\n",
        "                {\"type\" : \"text\", \"text\": messageT}\n",
        "            ]\n",
        "        }]\n",
        "        outputs =model.create_completion(prompt =json.dumps (messages) ,**generate_kwargs)\n",
        "    else:\n",
        "        outputs =model.create_completion(prompt =prompt ,**generate_kwargs)\n",
        "\n",
        "\n",
        "    for chunk in outputs:\n",
        "        #print (chunk )\n",
        "        content = chunk[\"choices\"][0][\"text\"]\n",
        "        #print(content)\n",
        "        if content:\n",
        "            partial_message+=content\n",
        "            yield partial_message\n",
        "    \"\"\"\n",
        "    for new_token in model.generate(model_inputs,**generate_kwargs):\n",
        "            #print (new_token)\n",
        "            if new_token in (213,7221,1):break\n",
        "            s=model.detokenize([new_token])\n",
        "            #print (s)\n",
        "            if isinstance(s,bytes):\n",
        "                try:partial_message += s.decode(\"utf-8\")\n",
        "                except :break\n",
        "            yield partial_message\n",
        "        \"\"\"\n",
        "\n",
        "chatbot = gr.Chatbot(likeable=True,show_copy_button=True)\n",
        "textbox = gr.Textbox (show_copy_button=True)\n",
        "\n",
        "user=\"brian\" #@param {type:\"string\"}\n",
        "password=\"pwd\" #@param {type:\"string\"}\n",
        "auth=(user,password)\n",
        "\n",
        "gr.ChatInterface(predict,chatbot=chatbot,stop_btn=\"Stop\" ,retry_btn=\"Retry\",concurrency_limit=1,description=description,\n",
        "additional_inputs=[gr.Number(value=100,minimum =0,maximum =1000,precision=0,show_label=True ,label=\"Topic K\"),\n",
        "gr.Number(value=0.85,minimum =0,maximum =1,precision=3,show_label=True ,label=\"Topic P\"),\n",
        "gr.Number(value=0.85,minimum =0,maximum =1,precision=3,show_label=True ,label=\"Temperature\"),gr.Number(value=1.1,minimum =1,maximum =3,precision=3,show_label=True ,label=\"Repeat penalty\") ,gr.Number(value=4096,minimum =0,maximum =8192,precision=0,show_label=True ,label=\"max_tokens\") ],multimodal=True ).launch(debug=True,share=True ,inline=False,auth=auth )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNM3KtgolCzaFbvfaSzDz7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}